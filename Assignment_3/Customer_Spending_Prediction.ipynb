{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19254c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing Dependency\n",
    "!pip install snowflake-snowpark-python\n",
    "!pip install snowflake-connector-python\n",
    "!pip install ipykernel\n",
    "!pip install pyarrow == 10.0.1\n",
    "!pip install numpy\n",
    "!pip install scikit-learn\n",
    "!pip install pandas\n",
    "!pip install joblib\n",
    "!pip install cachetools\n",
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12dfba6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from snowflake.connector import connect\n",
    "from snowflake.snowpark import functions as F\n",
    "from snowflake.snowpark.session import Session\n",
    "from snowflake.snowpark import version as v\n",
    "\n",
    "connection_parameters = {\n",
    "   \n",
    "  \"account\": \"\",\n",
    "  \"user\": \"\",\n",
    "  \"password\": \"\",\n",
    "  \"warehouse\": \"\",\n",
    "  \"database\": \"\",\n",
    "  \"schema\": \"\"\n",
    "}\n",
    "\n",
    "session = Session.builder.configs(connection_parameters).create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f7c003f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(status='SNOWFLAKE_SAMPLE_DATA already exists, statement succeeded.')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating a database 'snowflake_sample_data' from sfc_samples.sample_data. Where in will have DDL amd DML rights\n",
    "session.sql('''create database if not exists snowflake_sample_data from share sfc_samples.sample_data''').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72749496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(status='TPCDS_XGBOOST already exists, statement succeeded.')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.sql('CREATE DATABASE IF NOT EXISTS tpcds_xgboost').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "faa9d948",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating tpcds_xgboost Database\n",
    "session.sql('CREATE DATABASE IF NOT EXISTS tpcds_xgboost').collect()\n",
    "\n",
    "#Creating schema tpcds_xgboost.demo\n",
    "session.sql('CREATE SCHEMA IF NOT EXISTS tpcds_xgboost.demo').collect()\n",
    "\n",
    "#creating two different warehouses for different workload\n",
    "session.sql(\"create or replace warehouse FE_AND_INFERENCE_WH with warehouse_size='3X-LARGE'\").collect()\n",
    "session.sql(\"create or replace warehouse snowpark_opt_wh with warehouse_size = 'MEDIUM' warehouse_type = 'SNOWPARK-OPTIMIZED'\").collect()\n",
    "session.sql(\"alter warehouse snowpark_opt_wh set max_concurrency_level = 1\").collect()\n",
    "\n",
    "#setting current warehouse to be FE_AND_INFERENCE_WH\n",
    "session.use_warehouse('FE_AND_INFERENCE_WH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d479760",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Snowflake provide different datasets size to work with. Will provide flexibility to choose the size \n",
    "TPCDS_SIZE_PARAM = 100\n",
    "SNOWFLAKE_SAMPLE_DB = 'SNOWFLAKE_SAMPLE_DATA' \n",
    "\n",
    "if TPCDS_SIZE_PARAM == 100: \n",
    "    TPCDS_SCHEMA = 'TPCDS_SF100TCL'\n",
    "elif TPCDS_SIZE_PARAM == 10:\n",
    "    TPCDS_SCHEMA = 'TPCDS_SF10TCL'\n",
    "else:\n",
    "    raise ValueError(\"Invalid TPCDS_SIZE_PARAM selection\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39433837",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing table content as spark dataframe\n",
    "store_sales = session.table(f'{SNOWFLAKE_SAMPLE_DB}.{TPCDS_SCHEMA}.store_sales')\n",
    "catalog_sales = session.table(f'{SNOWFLAKE_SAMPLE_DB}.{TPCDS_SCHEMA}.catalog_sales') \n",
    "web_sales = session.table(f'{SNOWFLAKE_SAMPLE_DB}.{TPCDS_SCHEMA}.web_sales') \n",
    "date = session.table(f'{SNOWFLAKE_SAMPLE_DB}.{TPCDS_SCHEMA}.date_dim')\n",
    "dim_stores = session.table(f'{SNOWFLAKE_SAMPLE_DB}.{TPCDS_SCHEMA}.store')\n",
    "customer = session.table(f'{SNOWFLAKE_SAMPLE_DB}.{TPCDS_SCHEMA}.customer')\n",
    "address = session.table(f'{SNOWFLAKE_SAMPLE_DB}.{TPCDS_SCHEMA}.customer_address')\n",
    "demo = session.table(f'{SNOWFLAKE_SAMPLE_DB}.{TPCDS_SCHEMA}.customer_demographics')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e822f050",
   "metadata": {},
   "source": [
    "# Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "327f8131",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aggregating data\n",
    "store_sales_agged = store_sales.group_by('ss_customer_sk').agg(F.sum('ss_sales_price').as_('total_sales'))\n",
    "web_sales_agged = web_sales.group_by('ws_bill_customer_sk').agg(F.sum('ws_sales_price').as_('total_sales'))\n",
    "catalog_sales_agged = catalog_sales.group_by('cs_bill_customer_sk').agg(F.sum('cs_sales_price').as_('total_sales'))\n",
    "\n",
    "#Renaming the columns\n",
    "store_sales_agged = store_sales_agged.rename('ss_customer_sk', 'customer_sk')\n",
    "web_sales_agged = web_sales_agged.rename('ws_bill_customer_sk', 'customer_sk')\n",
    "catalog_sales_agged = catalog_sales_agged.rename('cs_bill_customer_sk', 'customer_sk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed2d126e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stacking all the three aggregated tables to have one table\n",
    "total_sales = store_sales_agged.union_all(web_sales_agged)\n",
    "total_sales = total_sales.union_all(catalog_sales_agged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38618708",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aggregating to have unique customer_sk\n",
    "total_sales = total_sales.group_by('customer_sk').agg(F.sum('total_sales').as_('total_sales'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2202c723",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select set of columns from customer table\n",
    "customer = customer.select('c_customer_sk','c_current_hdemo_sk', 'c_current_addr_sk', 'c_customer_id', 'c_birth_year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4198207c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"CUSTOMER_SK\"  |\"C_CURRENT_HDEMO_SK\"  |\"C_CURRENT_ADDR_SK\"  |\"C_CUSTOMER_ID\"   |\"C_BIRTH_YEAR\"  |\"CA_ADDRESS_SK\"  |\"CA_ZIP\"  |\"CD_DEMO_SK\"  |\"CD_GENDER\"  |\"CD_MARITAL_STATUS\"  |\"CD_CREDIT_RATING\"  |\"CD_EDUCATION_STATUS\"  |\"CD_DEP_COUNT\"  |\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|2435782        |5105                  |22514086             |AAAAAAAAGMKCFCAA  |1954            |22514086         |NULL      |5105          |M            |D                    |Unknown             |Unknown                |0               |\n",
      "|2436098        |6723                  |2937496              |AAAAAAAACAMCFCAA  |1962            |2937496          |29089     |6723          |M            |S                    |Good                |Primary                |1               |\n",
      "|2437565        |262                   |8612839              |AAAAAAAANLBDFCAA  |1973            |8612839          |68828     |262           |F            |M                    |Good                |Advanced Degree        |0               |\n",
      "|2439554        |2264                  |49615959             |AAAAAAAACIJDFCAA  |1932            |49615959         |54854     |2264          |F            |S                    |Low Risk            |College                |0               |\n",
      "|2439639        |6840                  |11958928             |AAAAAAAAHNJDFCAA  |1978            |11958928         |76340     |6840          |F            |U                    |Good                |4 yr Degree            |1               |\n",
      "|2440635        |3426                  |27764093             |AAAAAAAALLNDFCAA  |1939            |27764093         |38828     |3426          |F            |D                    |High Risk           |Unknown                |0               |\n",
      "|2440729        |1012                  |26967204             |AAAAAAAAJBODFCAA  |1926            |26967204         |63003     |1012          |F            |M                    |Good                |2 yr Degree            |0               |\n",
      "|2441346        |5697                  |13031561             |AAAAAAAACIAEFCAA  |1958            |13031561         |57057     |5697          |M            |W                    |Good                |College                |1               |\n",
      "|2442525        |6160                  |48810954             |AAAAAAAANBFEFCAA  |NULL            |48810954         |58883     |6160          |F            |U                    |Good                |Unknown                |1               |\n",
      "|2442537        |6721                  |49501253             |AAAAAAAAJCFEFCAA  |1953            |49501253         |35124     |6721          |M            |M                    |Good                |Primary                |1               |\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Joining address and demograhic table with cutomer table to create a custom customer table\n",
    "customer = customer.join(address.select('ca_address_sk', 'ca_zip'), customer['c_current_addr_sk'] == address['ca_address_sk'] )\n",
    "customer = customer.join(demo.select('cd_demo_sk', 'cd_gender', 'cd_marital_status', 'cd_credit_rating', 'cd_education_status', 'cd_dep_count'),\n",
    "                                customer['c_current_hdemo_sk'] == demo['cd_demo_sk'] )\n",
    "customer = customer.rename('c_customer_sk', 'customer_sk')\n",
    "customer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07746f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally joining aggregated sales and customer table\n",
    "final_df = total_sales.join(customer, on='customer_sk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa635c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing final table in Snowflake\n",
    "session.use_database('tpcds_xgboost')\n",
    "session.use_schema('demo')\n",
    "final_df.write.mode('overwrite').save_as_table('feature_store')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1b823da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The version of package snowflake-snowpark-python in the local environment is 1.2.0, which does not fit the criteria for the requirement snowflake-snowpark-python. Your UDF might not work when the package version is different between the server and your local environment\n",
      "The version of package scikit-learn in the local environment is 1.2.2, which does not fit the criteria for the requirement scikit-learn. Your UDF might not work when the package version is different between the server and your local environment\n",
      "The version of package pandas in the local environment is 1.5.3, which does not fit the criteria for the requirement pandas. Your UDF might not work when the package version is different between the server and your local environment\n",
      "The version of package numpy in the local environment is 1.24.2, which does not fit the criteria for the requirement numpy. Your UDF might not work when the package version is different between the server and your local environment\n",
      "The version of package joblib in the local environment is 1.2.0, which does not fit the criteria for the requirement joblib. Your UDF might not work when the package version is different between the server and your local environment\n",
      "The version of package xgboost in the local environment is 1.7.4, which does not fit the criteria for the requirement xgboost. Your UDF might not work when the package version is different between the server and your local environment\n"
     ]
    }
   ],
   "source": [
    "session.add_packages('snowflake-snowpark-python', 'scikit-learn', 'pandas', 'numpy', 'joblib', 'cachetools', 'xgboost', 'joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ec5acc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(status='Stage area ML_MODELS successfully created.')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we are creating a stage called ml_models that can be used to store machine learning models that we will train \n",
    "session.sql('CREATE OR REPLACE STAGE ml_models ').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35effee",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6da5d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Necessary Library\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from xgboost import XGBRegressor\n",
    "import joblib\n",
    "import os\n",
    "import snowflake\n",
    "\n",
    "#Model\n",
    "def train_model(session: snowflake.snowpark.Session) -> float:\n",
    "    snowdf = session.table(\"feature_store\")\n",
    "    #Dropping unwanted columns\n",
    "    snowdf = snowdf.drop(['CUSTOMER_SK', 'C_CURRENT_HDEMO_SK', 'C_CURRENT_ADDR_SK', 'C_CUSTOMER_ID', 'CA_ADDRESS_SK', 'CD_DEMO_SK'])\n",
    "    snowdf_train, snowdf_test = snowdf.random_split([0.8, 0.2], seed=82) \n",
    "\n",
    "    # save the train and test sets as time stamped tables in Snowflake \n",
    "    snowdf_train.write.mode(\"overwrite\").save_as_table(\"tpcds_xgboost.demo.tpc_TRAIN\")\n",
    "    snowdf_test.write.mode(\"overwrite\").save_as_table(\"tpcds_xgboost.demo.tpc_TEST\")\n",
    "    train_x = snowdf_train.drop(\"TOTAL_SALES\").to_pandas() # drop labels for training set\n",
    "    train_y = snowdf_train.select(\"TOTAL_SALES\").to_pandas()\n",
    "    test_x = snowdf_test.drop(\"TOTAL_SALES\").to_pandas()\n",
    "    test_y = snowdf_test.select(\"TOTAL_SALES\").to_pandas()\n",
    "    cat_cols = ['CA_ZIP', 'CD_GENDER', 'CD_MARITAL_STATUS', 'CD_CREDIT_RATING', 'CD_EDUCATION_STATUS']\n",
    "    num_cols = ['C_BIRTH_YEAR', 'CD_DEP_COUNT']\n",
    "\n",
    "    #Transformer to handle numerical values\n",
    "    num_pipeline = Pipeline([('imputer', SimpleImputer(strategy=\"median\")),('std_scaler', StandardScaler()),])\n",
    "\n",
    "    #Processing numerical and categorical columns\n",
    "    preprocessor = ColumnTransformer(\n",
    "    transformers=[('num', num_pipeline, num_cols),('encoder', OneHotEncoder(handle_unknown=\"ignore\"), cat_cols) ])\n",
    "\n",
    "    #pipline for dataprocessinga and model fitting\n",
    "    pipe = Pipeline([('preprocessor', preprocessor), ('xgboost', XGBRegressor())])\n",
    "    pipe.fit(train_x, train_y)\n",
    "\n",
    "    #model evaluation\n",
    "    test_preds = pipe.predict(test_x)\n",
    "    rmse = mean_squared_error(test_y, test_preds)\n",
    "    mape = mean_absolute_percentage_error(test_y, test_preds)\n",
    "\n",
    "    \n",
    "    #storing trained model into snowflake staging area\n",
    "    model_file = os.path.join('/tmp', 'model.joblib')\n",
    "    joblib.dump(pipe, model_file)\n",
    "    session.file.put(model_file, \"@ml_models\",overwrite=True)\n",
    "    return mape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0962bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Necessary Library\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import joblib\n",
    "import os\n",
    "import snowflake\n",
    "\n",
    "#Model\n",
    "def train_model(session: snowflake.snowpark.Session) -> float:\n",
    "    snowdf = session.table(\"feature_store\")\n",
    "    #Dropping unwanted columns\n",
    "    snowdf = snowdf.drop(['CUSTOMER_SK', 'C_CURRENT_HDEMO_SK', 'C_CURRENT_ADDR_SK', 'C_CUSTOMER_ID', 'CA_ADDRESS_SK', 'CD_DEMO_SK'])\n",
    "    snowdf_train, snowdf_test = snowdf.random_split([0.8, 0.2], seed=82) \n",
    "\n",
    "    # save the train and test sets as time stamped tables in Snowflake \n",
    "    snowdf_train.write.mode(\"overwrite\").save_as_table(\"tpcds_xgboost.demo.tpc_TRAIN\")\n",
    "    snowdf_test.write.mode(\"overwrite\").save_as_table(\"tpcds_xgboost.demo.tpc_TEST\")\n",
    "    train_x = snowdf_train.drop(\"TOTAL_SALES\").to_pandas() # drop labels for training set\n",
    "    train_y = snowdf_train.select(\"TOTAL_SALES\").to_pandas()\n",
    "    test_x = snowdf_test.drop(\"TOTAL_SALES\").to_pandas()\n",
    "    test_y = snowdf_test.select(\"TOTAL_SALES\").to_pandas()\n",
    "    cat_cols = ['CA_ZIP', 'CD_GENDER', 'CD_MARITAL_STATUS', 'CD_CREDIT_RATING', 'CD_EDUCATION_STATUS']\n",
    "    num_cols = ['C_BIRTH_YEAR', 'CD_DEP_COUNT']\n",
    "\n",
    "    #Transformer to handle numerical values\n",
    "    num_pipeline = Pipeline([('imputer', SimpleImputer(strategy=\"median\")),('std_scaler', StandardScaler()),])\n",
    "\n",
    "    #Processing numerical and categorical columns\n",
    "    preprocessor = ColumnTransformer(\n",
    "    transformers=[('num', num_pipeline, num_cols),('encoder', OneHotEncoder(handle_unknown=\"ignore\"), cat_cols) ])\n",
    "\n",
    "    #pipline for dataprocessinga and model fitting\n",
    "    pipe = Pipeline([('preprocessor', preprocessor), ('linear', LinearRegression())])\n",
    "    pipe.fit(train_x, train_y)\n",
    "\n",
    "    #model evaluation\n",
    "    test_preds = pipe.predict(test_x)\n",
    "    # rmse = mean_squared_error(test_y, test_preds)\n",
    "    mape = mean_absolute_percentage_error(test_y, test_preds)\n",
    "\n",
    "    \n",
    "    #storing trained model into snowflake staging area\n",
    "    model_file = os.path.join('/tmp', 'model.joblib')\n",
    "    joblib.dump(pipe, model_file)\n",
    "    session.file.put(model_file, \"@ml_models\",overwrite=True)\n",
    "    return mape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44f736d4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7979973908907877"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training Model\n",
    "session.use_warehouse('snowpark_opt_wh')\n",
    "train_model_sp = F.sproc(train_model, session=session, replace=True, is_permanent=True, name=\"xgboost_sproc\", stage_location=\"@ml_models\")\n",
    "# Switch to Snowpark Optimized Warehouse for training and to run the stored proc\n",
    "train_model_sp(session=session)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e6274b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch back to feature engineering/inference warehouse\n",
    "session.use_warehouse('FE_AND_INFERENCE_WH')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccc13ee",
   "metadata": {},
   "source": [
    "# Prediction Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb99af97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import cachetools\n",
    "import joblib\n",
    "from snowflake.snowpark import types as T\n",
    "\n",
    "session.add_import(\"@ml_models/model.joblib\")  \n",
    "\n",
    "features = [ 'C_BIRTH_YEAR', 'CA_ZIP', 'CD_GENDER', 'CD_MARITAL_STATUS', 'CD_CREDIT_RATING', 'CD_EDUCATION_STATUS', 'CD_DEP_COUNT']\n",
    "\n",
    "@cachetools.cached(cache={})\n",
    "def read_file(filename):\n",
    "       import_dir = sys._xoptions.get(\"snowflake_import_directory\")\n",
    "       if import_dir:\n",
    "              with open(os.path.join(import_dir, filename), 'rb') as file:\n",
    "                     m = joblib.load(file)\n",
    "                     return m\n",
    "\n",
    "@F.pandas_udf(session=session, max_batch_size=10000, is_permanent=True, stage_location='@ml_models', replace=True, name=\"clv_xgboost_udf\")\n",
    "def predict(df:  T.PandasDataFrame[int, str, str, str, str, str, int]) -> T.PandasSeries[float]:\n",
    "       m = read_file('model.joblib')       \n",
    "       df.columns = features\n",
    "       return m.predict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6233eba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_df = session.table('feature_store')\n",
    "inference_df = inference_df.drop(['CUSTOMER_SK', 'C_CURRENT_HDEMO_SK', 'C_CURRENT_ADDR_SK', 'C_CUSTOMER_ID', 'CA_ADDRESS_SK', 'CD_DEMO_SK'])\n",
    "inputs = inference_df.drop(\"TOTAL_SALES\")\n",
    "\n",
    "snowdf_results = inference_df.select(*inputs,\n",
    "                    predict(*inputs).alias('PREDICTION'), \n",
    "                    (F.col('TOTAL_SALES')).alias('ACTUAL_SALES'))\n",
    "\n",
    "snowdf_results.write.mode('overwrite').save_as_table('predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9870997",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_kernel",
   "language": "python",
   "name": "my_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
